{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:34.639076Z",
     "iopub.status.busy": "2021-07-25T13:52:34.638656Z",
     "iopub.status.idle": "2021-07-25T13:52:34.665443Z",
     "shell.execute_reply": "2021-07-25T13:52:34.664103Z",
     "shell.execute_reply.started": "2021-07-25T13:52:34.639045Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:37.522363Z",
     "iopub.status.busy": "2021-07-25T13:52:37.521959Z",
     "iopub.status.idle": "2021-07-25T13:52:40.004848Z",
     "shell.execute_reply": "2021-07-25T13:52:40.003757Z",
     "shell.execute_reply.started": "2021-07-25T13:52:37.522318Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os \n",
    "df4=pd.read_csv(\"/kaggle/input/finaldata09/new0.9.csv\")\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "# do your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:40.007323Z",
     "iopub.status.busy": "2021-07-25T13:52:40.006879Z",
     "iopub.status.idle": "2021-07-25T13:52:40.030387Z",
     "shell.execute_reply": "2021-07-25T13:52:40.029067Z",
     "shell.execute_reply.started": "2021-07-25T13:52:40.007276Z"
    }
   },
   "outputs": [],
   "source": [
    "df4.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:43.277148Z",
     "iopub.status.busy": "2021-07-25T13:52:43.276663Z",
     "iopub.status.idle": "2021-07-25T13:52:43.285958Z",
     "shell.execute_reply": "2021-07-25T13:52:43.284727Z",
     "shell.execute_reply.started": "2021-07-25T13:52:43.277095Z"
    }
   },
   "outputs": [],
   "source": [
    "y=df4[\"prediction\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:45.938132Z",
     "iopub.status.busy": "2021-07-25T13:52:45.937584Z",
     "iopub.status.idle": "2021-07-25T13:52:46.069685Z",
     "shell.execute_reply": "2021-07-25T13:52:46.068339Z",
     "shell.execute_reply.started": "2021-07-25T13:52:45.938072Z"
    }
   },
   "outputs": [],
   "source": [
    "x=df4.iloc[:,0:170]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:53.544506Z",
     "iopub.status.busy": "2021-07-25T13:52:53.544145Z",
     "iopub.status.idle": "2021-07-25T13:52:53.896136Z",
     "shell.execute_reply": "2021-07-25T13:52:53.894846Z",
     "shell.execute_reply.started": "2021-07-25T13:52:53.544476Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "x_min_max_scale=  pd.DataFrame(MinMaxScaler().fit_transform(x))\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:55.303059Z",
     "iopub.status.busy": "2021-07-25T13:52:55.302632Z",
     "iopub.status.idle": "2021-07-25T13:52:55.509983Z",
     "shell.execute_reply": "2021-07-25T13:52:55.508784Z",
     "shell.execute_reply.started": "2021-07-25T13:52:55.302997Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_min_max_scale,y , test_size=0.5, random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:52:57.317977Z",
     "iopub.status.busy": "2021-07-25T13:52:57.317503Z",
     "iopub.status.idle": "2021-07-25T13:52:57.326799Z",
     "shell.execute_reply": "2021-07-25T13:52:57.325303Z",
     "shell.execute_reply.started": "2021-07-25T13:52:57.317947Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train=np.expand_dims(x_train, axis=2)\n",
    "x_test=np.expand_dims(x_test, axis=2)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:53:02.621548Z",
     "iopub.status.busy": "2021-07-25T13:53:02.621201Z",
     "iopub.status.idle": "2021-07-25T13:53:02.794004Z",
     "shell.execute_reply": "2021-07-25T13:53:02.792281Z",
     "shell.execute_reply.started": "2021-07-25T13:53:02.621517Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model2():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(16, 5, padding='same', activation='sigmoid', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.4))\n",
    "    m.add(Conv1D(16, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.41))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.53))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:53:07.802515Z",
     "iopub.status.busy": "2021-07-25T13:53:07.802117Z",
     "iopub.status.idle": "2021-07-25T13:53:07.81661Z",
     "shell.execute_reply": "2021-07-25T13:53:07.814885Z",
     "shell.execute_reply.started": "2021-07-25T13:53:07.802482Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model3():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(16, 5, padding='same', activation='sigmoid', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.4))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.41))\n",
    "    m.add(Conv1D(64, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.53))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:53:12.00344Z",
     "iopub.status.busy": "2021-07-25T13:53:12.002986Z",
     "iopub.status.idle": "2021-07-25T13:53:12.016262Z",
     "shell.execute_reply": "2021-07-25T13:53:12.014603Z",
     "shell.execute_reply.started": "2021-07-25T13:53:12.00339Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model4():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(16, 5, padding='same', activation='sigmoid', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.3))\n",
    "    m.add(Conv1D(16, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.41))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "   # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.53))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:53:17.355368Z",
     "iopub.status.busy": "2021-07-25T13:53:17.35494Z",
     "iopub.status.idle": "2021-07-25T13:53:17.368163Z",
     "shell.execute_reply": "2021-07-25T13:53:17.366632Z",
     "shell.execute_reply.started": "2021-07-25T13:53:17.355336Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model5():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(32, 5, padding='same', activation='relu', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.3))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.41))\n",
    "    m.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "   # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.53))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='relu'))\n",
    "    m.add(Dense(128, activation='relu'))\n",
    "    m.add(Dense(64, activation='relu'))\n",
    "    m.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:54:30.548773Z",
     "iopub.status.busy": "2021-07-25T13:54:30.548369Z",
     "iopub.status.idle": "2021-07-25T13:54:30.560589Z",
     "shell.execute_reply": "2021-07-25T13:54:30.559334Z",
     "shell.execute_reply.started": "2021-07-25T13:54:30.548742Z"
    }
   },
   "outputs": [],
   "source": [
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "\n",
    "def whole_model():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(128, 5, padding='same', activation='relu', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    m.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    m.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    # m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    # m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    # m.add(Dropout(drop_out))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(128, activation='relu'))\n",
    "    m.add(Dense(32, activation='relu'))\n",
    "    m.add(Dense(2, activation = 'softmax'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=opt,loss='mean_squared_error',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:53:31.238251Z",
     "iopub.status.busy": "2021-07-25T13:53:31.23787Z",
     "iopub.status.idle": "2021-07-25T13:53:31.252255Z",
     "shell.execute_reply": "2021-07-25T13:53:31.249035Z",
     "shell.execute_reply.started": "2021-07-25T13:53:31.238219Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "n_folds=5\n",
    "epochs=150\n",
    "batch_size=128\n",
    "pat = 5\n",
    "#this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5000, min_delta=0, verbose=1 , mode=\"auto\",baseline=None,restore_best_weights=False)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5000, verbose=1, factor=0.1, min_lr=0.00001)\n",
    "#define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
    "model_checkpoint = ModelCheckpoint('subjek1CNN.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "#define a function to fit the model\n",
    "def fit_and_evaluate(x_train, x_test, y_train, y_test, EPOCHS=epochs, BATCH_SIZE=batch_size):\n",
    "    model = None\n",
    "    model = whole_model() \n",
    "    results = model.fit(x_train, y_train,epochs=EPOCHS, batch_size=BATCH_SIZE,steps_per_epoch=10000,verbose=1,validation_data=(x_test, y_test),callbacks=[learning_rate_reduction,early_stopping,model_checkpoint], validation_split=0.5)   \n",
    "    acc=model.evaluate(x_test, y_test)\n",
    "    print(\"Val Score: \", model.evaluate(x_test, y_test))\n",
    "    print(\"Loss:\", acc[0], \"Accuracy:\", acc[1])\n",
    "    pred = model.predict(x_test)\n",
    "    pred_y = pred.argmax(axis=-1)\n",
    "    pred_y\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    print(accuracy)\n",
    "    return results,pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T13:54:35.269657Z",
     "iopub.status.busy": "2021-07-25T13:54:35.269277Z",
     "iopub.status.idle": "2021-07-25T13:54:35.449682Z",
     "shell.execute_reply": "2021-07-25T13:54:35.4475Z",
     "shell.execute_reply.started": "2021-07-25T13:54:35.269624Z"
    }
   },
   "outputs": [],
   "source": [
    "history= fit_and_evaluate(x_train,x_test,y_train,y_test,300,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-25T13:28:24.949606Z",
     "iopub.status.idle": "2021-07-25T13:28:24.950147Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(dpi=1200)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Accuracy scores')\n",
    "plt.plot(history.history['accuracy'],'go-')\n",
    "plt.plot(history.history['val_accuracy'],'ro-')\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()\n",
    "plt.savefig(\"acc.png\",dpi=1500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-25T13:28:24.951417Z",
     "iopub.status.idle": "2021-07-25T13:28:24.952221Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=1200)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Loss value')\n",
    "plt.plot(history.history['loss'],'go-')\n",
    "plt.plot(history.history['val_loss'],'ro-')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "plt.savefig(\"loss.png\",dpi=1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-25T13:28:24.953777Z",
     "iopub.status.idle": "2021-07-25T13:28:24.954758Z"
    }
   },
   "outputs": [],
   "source": [
    "n_folds=3\n",
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = [] \n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    t_x, val_x, t_y, val_y = train_test_split(x_train, y_train,test_size=0.3, random_state = np.random.randint(1,250,8)[0])\n",
    "    model_history.append(fit_and_evaluate(x_train, x_test, y_train, y_test, epochs, batch_size))\n",
    "   \n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
