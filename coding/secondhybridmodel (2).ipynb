{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T14:07:40.432312Z",
     "start_time": "2021-08-20T14:07:39.979524Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:50:53.601015Z",
     "iopub.status.busy": "2021-07-25T08:50:53.600633Z",
     "iopub.status.idle": "2021-07-25T08:50:56.861234Z",
     "shell.execute_reply": "2021-07-25T08:50:56.860422Z",
     "shell.execute_reply.started": "2021-07-25T08:50:53.600931Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "# do your work here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os \n",
    "df=pd.read_csv(\"/kaggle/input/finaldata09/new0.9.csv\")\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:50:59.666548Z",
     "iopub.status.busy": "2021-07-25T08:50:59.66618Z",
     "iopub.status.idle": "2021-07-25T08:51:01.033468Z",
     "shell.execute_reply": "2021-07-25T08:51:01.032306Z",
     "shell.execute_reply.started": "2021-07-25T08:50:59.666516Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ./logs/ \n",
    "!mkdir ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:04.427749Z",
     "iopub.status.busy": "2021-07-25T08:51:04.427346Z",
     "iopub.status.idle": "2021-07-25T08:51:04.458507Z",
     "shell.execute_reply": "2021-07-25T08:51:04.457375Z",
     "shell.execute_reply.started": "2021-07-25T08:51:04.427696Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:08.469952Z",
     "iopub.status.busy": "2021-07-25T08:51:08.46961Z",
     "iopub.status.idle": "2021-07-25T08:51:08.478285Z",
     "shell.execute_reply": "2021-07-25T08:51:08.477232Z",
     "shell.execute_reply.started": "2021-07-25T08:51:08.469918Z"
    }
   },
   "outputs": [],
   "source": [
    "y=df[\"prediction\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:14.876217Z",
     "iopub.status.busy": "2021-07-25T08:51:14.875874Z",
     "iopub.status.idle": "2021-07-25T08:51:14.985818Z",
     "shell.execute_reply": "2021-07-25T08:51:14.984817Z",
     "shell.execute_reply.started": "2021-07-25T08:51:14.876182Z"
    }
   },
   "outputs": [],
   "source": [
    "x=df.iloc[:,0:170]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:33.507825Z",
     "iopub.status.busy": "2021-07-25T08:51:33.507492Z",
     "iopub.status.idle": "2021-07-25T08:51:35.48096Z",
     "shell.execute_reply": "2021-07-25T08:51:35.480128Z",
     "shell.execute_reply.started": "2021-07-25T08:51:33.507791Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "x_min_max_scale=  pd.DataFrame(MinMaxScaler().fit_transform(x))\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:35.546296Z",
     "iopub.status.busy": "2021-07-25T08:51:35.545989Z",
     "iopub.status.idle": "2021-07-25T08:51:35.731835Z",
     "shell.execute_reply": "2021-07-25T08:51:35.730965Z",
     "shell.execute_reply.started": "2021-07-25T08:51:35.546268Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_min_max_scale,y , test_size=0.3, random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:39.52178Z",
     "iopub.status.busy": "2021-07-25T08:51:39.521388Z",
     "iopub.status.idle": "2021-07-25T08:51:39.527609Z",
     "shell.execute_reply": "2021-07-25T08:51:39.526743Z",
     "shell.execute_reply.started": "2021-07-25T08:51:39.521747Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train=np.expand_dims(x_train, axis=2)\n",
    "x_test=np.expand_dims(x_test, axis=2)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:41.312216Z",
     "iopub.status.busy": "2021-07-25T08:51:41.311808Z",
     "iopub.status.idle": "2021-07-25T08:51:41.381383Z",
     "shell.execute_reply": "2021-07-25T08:51:41.380554Z",
     "shell.execute_reply.started": "2021-07-25T08:51:41.312174Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense, TimeDistributed, Activation, BatchNormalization, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.utils import Sequence\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:49.883583Z",
     "iopub.status.busy": "2021-07-25T08:51:49.883209Z",
     "iopub.status.idle": "2021-07-25T08:51:49.899467Z",
     "shell.execute_reply": "2021-07-25T08:51:49.897971Z",
     "shell.execute_reply.started": "2021-07-25T08:51:49.883551Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model2():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(8, kernel_size = 10, strides = 1, activation='relu', input_shape=(170,1)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.4))\n",
    "    m.add(Conv1D(16, kernel_size = 10, strides = 1, activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    m.add(Conv1D(32, kernel_size = 10, strides = 1, activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    m.add(Conv1D(64, kernel_size = 10, strides = 1, activation='relu'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, return_state = False)))\n",
    "    m.add(Bidirectional(CuDNNLSTM(64, return_sequences = True, return_state = False)))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:51:54.459977Z",
     "iopub.status.busy": "2021-07-25T08:51:54.45962Z",
     "iopub.status.idle": "2021-07-25T08:51:54.47232Z",
     "shell.execute_reply": "2021-07-25T08:51:54.471241Z",
     "shell.execute_reply.started": "2021-07-25T08:51:54.459942Z"
    }
   },
   "outputs": [],
   "source": [
    "def cnn_model3():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(16, 5, padding='same', activation='sigmoid', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.4))\n",
    "    m.add(Conv1D(16, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(drop_out))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.6))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Bidirectional(CuDNNLSTM(128, return_sequences = True, return_state = False)))\n",
    "    m.add(Bidirectional(CuDNNLSTM(64, return_sequences = True, return_state = False)))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T08:52:30.474702Z",
     "iopub.status.busy": "2021-07-25T08:52:30.474366Z",
     "iopub.status.idle": "2021-07-25T08:52:30.485046Z",
     "shell.execute_reply": "2021-07-25T08:52:30.483841Z",
     "shell.execute_reply.started": "2021-07-25T08:52:30.474671Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "n_folds=5\n",
    "epochs=1\n",
    "batch_size=128\n",
    "pat = 5\n",
    "#this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5000, min_delta=0, verbose=1 , mode=\"auto\",baseline=None,restore_best_weights=False)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5000, verbose=1, factor=0.1, min_lr=0.00001)\n",
    "#define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
    "model_checkpoint = ModelCheckpoint('subjek1CNN.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "#define a function to fit the model\n",
    "def fit_and_evaluate(x_train, x_test, y_train, y_test, EPOCHS=epochs, BATCH_SIZE=batch_size):\n",
    "    model = None\n",
    "    model = cnn_model4() \n",
    "    results = model.fit(x_train, y_train,epochs=EPOCHS, batch_size=BATCH_SIZE,steps_per_epoch=10000,verbose=1,validation_data=(x_test, y_test),callbacks=[learning_rate_reduction,early_stopping,model_checkpoint], validation_split=0.3)   \n",
    "    acc=model.evaluate(x_test, y_test)\n",
    "    print(\"Val Score: \", model.evaluate(x_test, y_test))\n",
    "    print(\"Loss:\", acc[0], \"Accuracy:\", acc[1])\n",
    "    pred = model.predict(x_test)\n",
    "    pred_y = pred.argmax(axis=-1)\n",
    "    pred_y\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    print(accuracy)\n",
    "    return results,pred_y,pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history,pred_y= fit_and_evaluate(x_train,x_test,y_train,y_test,150,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cnn_model4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn import metrics \n",
    "plt.figure(dpi=1500)\n",
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "plt.savefig(\"accuracy.png\",dpi=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=1280)\n",
    "\n",
    "pre, rec, thr = metrics.precision_recall_curve(y_test, prob_test)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(thr, pre[:-1], label='precision')\n",
    "plt.plot(thr, rec[1:], label='recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.title('Precision & Recall vs Threshold', c='r', size=16)\n",
    "plt.legend()\n",
    "plt.savefig('Precision&RecallvsThreshold.png', dpi=1500)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=3\n",
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = [] \n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    t_x, val_x, t_y, val_y = train_test_split(x_train, y_train,test_size=0.3, random_state = np.random.randint(1,250,8)[0])\n",
    "    model_history.append(fit_and_evaluate(x_train, x_test, y_train, y_test, epochs, batch_size))\n",
    "   \n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
