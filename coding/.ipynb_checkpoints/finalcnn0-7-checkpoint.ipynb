{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:43.630128Z",
     "iopub.status.busy": "2021-07-25T14:10:43.629801Z",
     "iopub.status.idle": "2021-07-25T14:10:43.64601Z",
     "shell.execute_reply": "2021-07-25T14:10:43.644972Z",
     "shell.execute_reply.started": "2021-07-25T14:10:43.630097Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:43.647559Z",
     "iopub.status.busy": "2021-07-25T14:10:43.647307Z",
     "iopub.status.idle": "2021-07-25T14:10:47.078459Z",
     "shell.execute_reply": "2021-07-25T14:10:47.077598Z",
     "shell.execute_reply.started": "2021-07-25T14:10:43.647535Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os \n",
    "df3=pd.read_csv(\"/kaggle/input/finaldata07/new0.7.csv\")\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "# do your work here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:47.081167Z",
     "iopub.status.busy": "2021-07-25T14:10:47.080559Z",
     "iopub.status.idle": "2021-07-25T14:10:47.141032Z",
     "shell.execute_reply": "2021-07-25T14:10:47.140074Z",
     "shell.execute_reply.started": "2021-07-25T14:10:47.081129Z"
    }
   },
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:47.143094Z",
     "iopub.status.busy": "2021-07-25T14:10:47.142739Z",
     "iopub.status.idle": "2021-07-25T14:10:47.167725Z",
     "shell.execute_reply": "2021-07-25T14:10:47.16679Z",
     "shell.execute_reply.started": "2021-07-25T14:10:47.143057Z"
    }
   },
   "outputs": [],
   "source": [
    "df3.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:47.169539Z",
     "iopub.status.busy": "2021-07-25T14:10:47.169174Z",
     "iopub.status.idle": "2021-07-25T14:10:47.177177Z",
     "shell.execute_reply": "2021-07-25T14:10:47.176363Z",
     "shell.execute_reply.started": "2021-07-25T14:10:47.169503Z"
    }
   },
   "outputs": [],
   "source": [
    "y=df3[\"prediction\"]\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:47.179298Z",
     "iopub.status.busy": "2021-07-25T14:10:47.17863Z",
     "iopub.status.idle": "2021-07-25T14:10:47.334078Z",
     "shell.execute_reply": "2021-07-25T14:10:47.333092Z",
     "shell.execute_reply.started": "2021-07-25T14:10:47.179246Z"
    }
   },
   "outputs": [],
   "source": [
    "x=df3.iloc[:,0:170]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:47.33722Z",
     "iopub.status.busy": "2021-07-25T14:10:47.336647Z",
     "iopub.status.idle": "2021-07-25T14:10:47.790917Z",
     "shell.execute_reply": "2021-07-25T14:10:47.79003Z",
     "shell.execute_reply.started": "2021-07-25T14:10:47.337176Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "x_min_max_scale=  pd.DataFrame(MinMaxScaler().fit_transform(x))\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:47.792698Z",
     "iopub.status.busy": "2021-07-25T14:10:47.79243Z",
     "iopub.status.idle": "2021-07-25T14:10:48.110371Z",
     "shell.execute_reply": "2021-07-25T14:10:48.109474Z",
     "shell.execute_reply.started": "2021-07-25T14:10:47.792673Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_min_max_scale,y , test_size=0.5, random_state=42) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:48.112167Z",
     "iopub.status.busy": "2021-07-25T14:10:48.111642Z",
     "iopub.status.idle": "2021-07-25T14:10:48.118728Z",
     "shell.execute_reply": "2021-07-25T14:10:48.117808Z",
     "shell.execute_reply.started": "2021-07-25T14:10:48.112128Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train=np.expand_dims(x_train, axis=2)\n",
    "x_test=np.expand_dims(x_test, axis=2)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:48.120587Z",
     "iopub.status.busy": "2021-07-25T14:10:48.120162Z",
     "iopub.status.idle": "2021-07-25T14:10:48.135993Z",
     "shell.execute_reply": "2021-07-25T14:10:48.134589Z",
     "shell.execute_reply.started": "2021-07-25T14:10:48.120546Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model2():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(16, 5, padding='same', activation='sigmoid', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.4))\n",
    "    m.add(Conv1D(16, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.41))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.53))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:48.138039Z",
     "iopub.status.busy": "2021-07-25T14:10:48.137526Z",
     "iopub.status.idle": "2021-07-25T14:10:48.15175Z",
     "shell.execute_reply": "2021-07-25T14:10:48.150799Z",
     "shell.execute_reply.started": "2021-07-25T14:10:48.137998Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model3():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(16, 5, padding='same', activation='sigmoid', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.4))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.41))\n",
    "    m.add(Conv1D(64, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.53))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:48.153632Z",
     "iopub.status.busy": "2021-07-25T14:10:48.15307Z",
     "iopub.status.idle": "2021-07-25T14:10:48.166529Z",
     "shell.execute_reply": "2021-07-25T14:10:48.16552Z",
     "shell.execute_reply.started": "2021-07-25T14:10:48.153589Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D,Dropout,BatchNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "def cnn_model4():\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(16, 5, padding='same', activation='sigmoid', input_shape=(170, 1)))\n",
    "    m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.3))\n",
    "    m.add(Conv1D(16, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.41))\n",
    "    m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    m.add(BatchNormalization())\n",
    "   # m.add(MaxPooling1D(pool_size=2))\n",
    "    m.add(Dropout(0.42))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='sigmoid'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.53))\n",
    "    #m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    #m.add(BatchNormalization())\n",
    "    #m.add(MaxPooling1D(pool_size=2))\n",
    "    #m.add(Dropout(0.64))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(256, activation='sigmoid'))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dense(64, activation='sigmoid'))\n",
    "    m.add(Dense(32, activation='sigmoid'))\n",
    "    \n",
    "    m.add(Dense(2, activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
    "   # m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:48.168635Z",
     "iopub.status.busy": "2021-07-25T14:10:48.168211Z",
     "iopub.status.idle": "2021-07-25T14:10:48.181224Z",
     "shell.execute_reply": "2021-07-25T14:10:48.18021Z",
     "shell.execute_reply.started": "2021-07-25T14:10:48.168597Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, AveragePooling1D, MaxPooling1D, TimeDistributed, LeakyReLU, BatchNormalization, Flatten\n",
    "from keras import optimizers, callbacks\n",
    "from keras.regularizers import l2\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "LR = 0.0009 # maybe after some (10-15) epochs reduce it to 0.0008-0.0007\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 35\n",
    "\n",
    "#loss = 'categorical_hinge' # ok\n",
    "loss = 'categorical_crossentropy' # best standart\n",
    "#loss = 'mean_absolute_error' # bad\n",
    "#loss = 'mean_squared_logarithmic_error' # new best (a little better)\n",
    "\n",
    "\n",
    "def whole_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, 5, padding='same', activation='relu', input_shape=(170, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(drop_out))\n",
    "    # m.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    # m.add(BatchNormalization())\n",
    "    # m.add(MaxPooling1D(pool_size=2))\n",
    "    # m.add(Dropout(drop_out))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=LR)\n",
    "    model.compile(optimizer=opt,loss=\"mean_squared_error\",metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:48.182876Z",
     "iopub.status.busy": "2021-07-25T14:10:48.182539Z",
     "iopub.status.idle": "2021-07-25T14:10:48.194395Z",
     "shell.execute_reply": "2021-07-25T14:10:48.193129Z",
     "shell.execute_reply.started": "2021-07-25T14:10:48.18284Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "n_folds=5\n",
    "epochs=150\n",
    "batch_size=128\n",
    "pat = 5\n",
    "#this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5000, min_delta=0, verbose=1 , mode=\"auto\",baseline=None,restore_best_weights=False)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5000, verbose=1, factor=0.1, min_lr=0.00001)\n",
    "#define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
    "model_checkpoint = ModelCheckpoint('subjek1CNN.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "#define a function to fit the model\n",
    "def fit_and_evaluate(x_train, x_test, y_train, y_test, EPOCHS=epochs, BATCH_SIZE=batch_size):\n",
    "    \n",
    "    model = whole_model() \n",
    "    results = model.fit(x_train, y_train,epochs=EPOCHS, batch_size=BATCH_SIZE,steps_per_epoch=10000,verbose=1,validation_data=(x_test, y_test),callbacks=[learning_rate_reduction,early_stopping,model_checkpoint], validation_split=0.3)   \n",
    "    acc=model.evaluate(x_test, y_test)\n",
    "    print(\"Val Score: \", model.evaluate(x_test, y_test))\n",
    "    print(\"Loss:\", acc[0], \"Accuracy:\", acc[1])\n",
    "    pred = model.predict(x_test)\n",
    "    pred_y = pred.argmax(axis=-1)\n",
    "    pred_y\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    print(accuracy)\n",
    "    return results,pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:10:48.196Z",
     "iopub.status.busy": "2021-07-25T14:10:48.195603Z",
     "iopub.status.idle": "2021-07-25T14:34:07.669949Z",
     "shell.execute_reply": "2021-07-25T14:34:07.669004Z",
     "shell.execute_reply.started": "2021-07-25T14:10:48.195961Z"
    }
   },
   "outputs": [],
   "source": [
    "history,pred_y= fit_and_evaluate(x_train,x_test,y_train,y_test,300,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T14:34:07.671593Z",
     "iopub.status.busy": "2021-07-25T14:34:07.671232Z",
     "iopub.status.idle": "2021-07-25T15:17:15.171151Z",
     "shell.execute_reply": "2021-07-25T15:17:15.169621Z",
     "shell.execute_reply.started": "2021-07-25T14:34:07.671555Z"
    }
   },
   "outputs": [],
   "source": [
    "n_folds=3\n",
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = [] \n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    t_x, val_x, t_y, val_y = train_test_split(x_train, y_train,test_size=0.3, random_state = np.random.randint(1,250,8)[0])\n",
    "    model_history.append(fit_and_evaluate(x_train, x_test, y_train, y_test, epochs, batch_size))\n",
    "   \n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
